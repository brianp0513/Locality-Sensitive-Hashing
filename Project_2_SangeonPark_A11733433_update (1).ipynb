{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Project_2_SangeonPark_A11733433_update.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhAu5PpJfpUo"
      },
      "source": [
        "# ENTER YOUR NAME AND ID HERE\n",
        "# Sangeon Park\n",
        "# A11733433"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VXduI5zfpUq"
      },
      "source": [
        "# Project-2: Locality Sensitive Hashing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcOLFzsofpUr"
      },
      "source": [
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import regex\n",
        "import sys\n",
        "from functools import reduce\n",
        "\n",
        "from random import randrange\n",
        "from shutil import copyfile"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXvwUX3ufpUs"
      },
      "source": [
        "Suspicious datasets for this project will be in <b>Suspicious</b> directory\n",
        "\n",
        "\n",
        "Your query datasets will be in <b>Original</b> directory\n",
        "\n",
        "\n",
        "You have to use any one original Wikipedia article from <b>Original</b> for <b>Fact Checks</b> steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-itt8BAfpUs",
        "outputId": "12adb4d7-c472-43eb-dc14-941c49c903e1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5tYasNfgXUl"
      },
      "source": [
        "path = '/content/drive/MyDrive/Data/corpus-20090418'\n",
        "\n",
        "os.makedirs('Suspicious_Sample')\n",
        "os.makedirs('Original_Sample')\n",
        "# you don't have to run this it is just organize original and suspicious files into seperate directory.\n",
        "for file in os.listdir(path):\n",
        "  if 'orig' in file:\n",
        "    copyfile(path + '/'+file,'Original_Sample/'+file)\n",
        "  else: \n",
        "    copyfile(path + '/'+file,'Suspicious_Sample/'+file)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qddrn4JWfpUs"
      },
      "source": [
        "### STEP - 1: Shingling (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R0IbKSCfpUt"
      },
      "source": [
        "# Type your code here... \n",
        "# Create necessary number of cells below this cell\n",
        "susdir = '/content/Suspicious_Sample'\n",
        "oridir = '/content/Original_Sample'\n",
        "preprocessing_suslist = []\n",
        "filename_suslist = []\n",
        "\n",
        "susfilelist = os.listdir(susdir)\n",
        "# print(susfilelist)\n",
        "for file in sorted(susfilelist):\n",
        "  with open(susdir+\"/\"+file,'r',encoding = \"ISO-8859-1\") as file1:\n",
        "    file1 = file1.read()\n",
        "    preprocessing = file1.replace('\\n','')\n",
        "    # Switch upper->lower\n",
        "    preprocessing = file1.lower()\n",
        "    preprocessing = re.sub('[^a-z0-9 ]+ ','',preprocessing)\n",
        "\n",
        "    # # remove special character\n",
        "    pattern = r'[' + string.punctuation + ']'\n",
        "    preprocessing = re.sub(pattern,'',preprocessing)\n",
        "    # # remove latin word\n",
        "    preprocessing = re.sub(r'[^\\x00-\\x7f]',r'',preprocessing)\n",
        "    # # print(file ,preprocessing)\n",
        "    # one document preprocessing done\n",
        "    preprocessing_suslist.append(preprocessing)\n",
        "  # appending file name into list\n",
        "  filename_suslist.append(file)\n",
        "# print(filename_suslist)\n",
        "# print(preprocessing_suslist)\n",
        "# NOTE: No complex text processing is required\n",
        "# convert just upper case characters to lower case\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAtcvb6QfpUt"
      },
      "source": [
        "def gettotalKshingles(preprocessing_suslist,k):\n",
        "  Kshingles_total= set()\n",
        "  # for loop for reading single suspicious \n",
        "  for single_sus in preprocessing_suslist:\n",
        "    # split by word split default is space. \n",
        "    split_word = single_sus.split()\n",
        "    shingles = []\n",
        "    # run list by len(split_word)-k-1. -k-1 is for prevent out of bounds index\n",
        "    for i in range(0,len(split_word)-k-1):\n",
        "      # grouping the words based on value of k\n",
        "      shingles.append(tuple(split_word[i:i+k]))\n",
        "    # changing to set remove duplicate k shingle  \n",
        "    shingles = set(shingles)\n",
        "    # update(add) into total list\n",
        "    Kshingles_total.update(shingles)\n",
        "    #return total k-shingles for all document\n",
        "  return Kshingles_total\n",
        "\n",
        "# \n",
        "def getKshingles_perfile(preprocessing_suslist,k):\n",
        "  Kshingles_perfile= list()\n",
        "  # for loop for reading single suspicious \n",
        "  for single_sus in preprocessing_suslist:\n",
        "    # split by word split default is space. \n",
        "    split_word = single_sus.split()\n",
        "    shingles = []\n",
        "    # run list by len(split_word)-k-1. -k-1 is for prevent out of bounds index\n",
        "    for i in range(0,len(split_word)-k-1):\n",
        "      # grouping the words based on value of k\n",
        "      shingles.append(tuple(split_word[i:i+k]))\n",
        "    # changing to set remove duplicate k shingle  \n",
        "    shingles = set(shingles)\n",
        "    # append(add) into total list. it can save seperately by document. Above function is for gathering all shingles in one document. \n",
        "    Kshingles_perfile.append(shingles)\n",
        "  return Kshingles_perfile\n",
        "\n",
        "# print(\"k=3\", len(gettotalKshingles(preprocessing_suslist,3)))\n",
        "# print(\"k=4\", len(gettotalKshingles(preprocessing_suslist,4)))\n",
        "# print(\"k=5\", len(gettotalKshingles(preprocessing_suslist,5)))\n",
        "# print(gettotalKshingles(preprocessing_suslist,5))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "mefOBY2IfpUt"
      },
      "source": [
        "Report results (number of unique k-shingles) for k={3,4,5} below:\n",
        "1. k=3:\n",
        "2. k=4:\n",
        "3. k=5:\n",
        "\n",
        "k=3 12249\n",
        "k=4 13317\n",
        "k=5 13806"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1GcLtV8fpUu"
      },
      "source": [
        "# Type your code to get the 5-shingle index here\n",
        "# to build signature matrix\n",
        "shingles_total = gettotalKshingles(preprocessing_suslist,5)\n",
        "Shingles5_with_index = dict()\n",
        "# provide shingles with index\n",
        "for index, value in enumerate(shingles_total):\n",
        "  Shingles5_with_index[index] = value\n",
        "# print(Shingles5_with_index)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVvlhwNWfpUu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64l9jkOZfpUv"
      },
      "source": [
        "### STEP - 2: Min-Hashing (40 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFn7q0cEfpUv"
      },
      "source": [
        "# ***************************************************NEW***********************************************************\n",
        "# Generate Hash functions - \n",
        "    # We use (ax + b) mod N formula to permute shingle index\n",
        "    # where a,b are random numbers, N total index size, and x is the index\n",
        "# We need to do L permutations - In other words we need to have L permutations (lists) of new indexes\n",
        "# Following function takes total index size N and L as arguments\n",
        "    # And returns L new lists of size N    \n",
        "def get_hash_functions(N,L):\n",
        "    hash_functions = []\n",
        "    \n",
        "    for itr in range(L):\n",
        "        a=randrange(1,400)\n",
        "        b=randrange(1,400)\n",
        "        \n",
        "        new_hash_function = []\n",
        "        for i in range(N):\n",
        "            new_hash_function.append((a * i + b) % N)\n",
        "        \n",
        "        hash_functions.append(new_hash_function)\n",
        "    return hash_functions\n",
        "        \n",
        "# test\n",
        "# hash_functions = get_hash_functions(5000,50)\n",
        "# print(hash_functions)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3gvWDwtfpUv"
      },
      "source": [
        "# Type your code here to generate all L hash functions\n",
        "# Generate hash functions only for shingle index created for k=5\n",
        "L = {50,100,200,500,1000}\n",
        "N = len(shingles_total)\n",
        "hash_functions_list = []\n",
        "\n",
        "# permutation\n",
        "# I used the for loop for appending and run get_hash_function into list but some reason it randomly mixed up. \n",
        "hash_functions_list.append(get_hash_functions(N,50))\n",
        "hash_functions_list.append(get_hash_functions(N,100))\n",
        "hash_functions_list.append(get_hash_functions(N,200))\n",
        "hash_functions_list.append(get_hash_functions(N,500))\n",
        "hash_functions_list.append(get_hash_functions(N,1000))\n",
        "\n",
        "\n",
        "test_hash = get_hash_functions(N,50)\n",
        "# print(hash_functions50)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zoeztgvfpUw"
      },
      "source": [
        "# print(test_hash)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrKDMFo7fpUw"
      },
      "source": [
        "# Type your code here to get the final signature matrix S here\n",
        "# input matrix\n",
        "################################# whatever commented code is testcase for one loop before I build def version #########################################\n",
        "# f = open('stdout2.txt', 'w')\n",
        "# shingles_per_file = getKshingles_perfile(preprocessing_suslist,5)\n",
        "# shingles_total = gettotalKshingles(preprocessing_suslist,5)\n",
        "# inputmatrix_perfile = []\n",
        "# signatrue_matrix=np.full((len(test_hash),len(shingles_per_file)),9999999999)\n",
        "\n",
        "\n",
        "# for col,fileShingles in enumerate(shingles_per_file):\n",
        "#   for i in shingles_total:\n",
        "#     temp = {i}\n",
        "#     if temp.issubset(fileShingles):\n",
        "#       inputmatrix_perfile.append(1)\n",
        "#     else:\n",
        "#       inputmatrix_perfile.append(0)\n",
        "#   for index ,onebitvector in enumerate(inputmatrix_perfile):\n",
        "#     if onebitvector ==1:\n",
        "#       for row, hash in enumerate(test_hash):\n",
        "#         if hash[index] < signatrue_matrix[row][col]:\n",
        "#           signatrue_matrix[row][col] = hash[index]\n",
        "#   inputmatrix_perfile = []\n",
        "\n",
        "# np.set_printoptions(threshold=np.inf)\n",
        "# print(inputmatrix_perfile)\n",
        "# print(signatrue_matrix,file=f)\n",
        "# f.close()\n",
        "\n",
        "def makeSignatureMatrix(shingles_per_file,shingles_total,All_hash):\n",
        "  # shingles_per_file = getKshingles_perfile(preprocessing_suslist,5)\n",
        "  # shingles_total = gettotalKshingles(preprocessing_suslist,5)\n",
        "  inputmatrix_perfile = []\n",
        "  signatrue_matrix=np.full((len(All_hash),len(shingles_per_file)),9999999999)\n",
        "\n",
        "  for col,fileShingles in enumerate(shingles_per_file):#loop by documents(95)\n",
        "    for i in shingles_total:\n",
        "      # i is tuple type. put i in set\n",
        "      temp = {i}\n",
        "      # if temp(one shingles) in one file shingles list append 1 otherwise append 0 into list.\n",
        "      if temp.issubset(fileShingles):\n",
        "        inputmatrix_perfile.append(1)\n",
        "      else:\n",
        "        inputmatrix_perfile.append(0)\n",
        "    # after complete one document inputmatrix(1,amount of shingles for one document). run that inputmatrix\n",
        "    for index ,onebitvector in enumerate(inputmatrix_perfile):\n",
        "      if onebitvector ==1:\n",
        "        # loop the permutation for inputmatrix status is 1\n",
        "        for row, hash in enumerate(All_hash):\n",
        "          # compare with the signature matrix value(inital should large int), if it find the smaller value from the permutation than signature value \n",
        "          if hash[index] < signatrue_matrix[row][col]:\n",
        "            # replace it\n",
        "            signatrue_matrix[row][col] = hash[index]\n",
        "    # After finish to make signature matrix for one document, reset the one document input matrix\n",
        "    inputmatrix_perfile = []\n",
        "  return signatrue_matrix\n",
        "\n",
        "# f = open('stdout3.txt', 'w')\n",
        "# np.savetxt('test_hash.txt',signatrue_matrix,delimiter=',')\n",
        "# np.set_printoptions(threshold=np.inf)\n",
        "# result = makeSignatureMatrix(test_hash)\n",
        "# print(result,file=f)\n",
        "signature_matrix_list=[]\n",
        "shingles_per_file = getKshingles_perfile(preprocessing_suslist,5)\n",
        "shingles_total = gettotalKshingles(preprocessing_suslist,5)\n",
        "for i in hash_functions_list:\n",
        "  signature_matrix_list.append(makeSignatureMatrix(shingles_per_file,shingles_total,i))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLYcRNDyopfQ"
      },
      "source": [
        "# a = np.full((2,3),1)\n",
        "# print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuKdkkNkscr0"
      },
      "source": [
        "(50, 95)\n",
        "50 = row\n",
        "95 = colum\n",
        "\n",
        "(2,3)\n",
        "[[1 1 1]\n",
        " [1 1 1]]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWF-YJxaiQiM"
      },
      "source": [
        "# Type your code here to do the fact check \n",
        "#      with any one query document in the 'Original' directory\n",
        "############## this process is exacly same that when I preprocessing and append into list in suspicious list \n",
        "susdir = '/content/Suspicious_Sample'\n",
        "oridir = '/content/Original_Sample'\n",
        "preprocessing_orilist = []\n",
        "filename_orilist = []\n",
        "\n",
        "orifilelist = os.listdir(oridir)\n",
        "# print(susfilelist)\n",
        "for file in sorted(orifilelist):\n",
        "  with open(oridir+\"/\"+file,'r',encoding = \"ISO-8859-1\") as file1:\n",
        "    file1 = file1.read()\n",
        "    file1 = file1.replace('\\n','')\n",
        "    # Switch upper->lower\n",
        "    preprocessing = file1.lower()\n",
        "    preprocessing = re.sub('[^a-z0-9 ]+ ','',preprocessing)\n",
        "    # # remove special character\n",
        "    pattern = r'[' + string.punctuation + ']'\n",
        "    preprocessing = re.sub(pattern,'',preprocessing)\n",
        "    # # remove latin word\n",
        "    preprocessing = re.sub(r'[^\\x00-\\x7f]',r'',preprocessing)\n",
        "    preprocessing_orilist.append(preprocessing)\n",
        "  filename_orilist.append(file)\n",
        "# print(filename_orilist)\n",
        "# print(preprocessing_orilist)\n",
        "\n",
        "# STEP-1: Generate 5-shingles \n",
        "    # (if any shingles are not present in your shingle index, simply ignore them)\n",
        "# this process is same as when program make suspcious shingles\n",
        "total_orginal_five_shingles = gettotalKshingles(preprocessing_orilist,5)\n",
        "per_file_original_five_shingles = getKshingles_perfile(preprocessing_orilist,5)\n",
        "# print(per_file_original_five_shingles)    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# STEP-2: Generate signature vector from L hash functions\n",
        "original_signature_list = []\n",
        "# this process is exactly same as when program create suspicous signature matrix \n",
        "for i in hash_functions_list:\n",
        "  original_signature_list.append(makeSignatureMatrix(per_file_original_five_shingles,shingles_total,i))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl5e5gKcfpUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7a48320-f95c-413c-afbe-16d2cf2078ba"
      },
      "source": [
        "# STEP-3: Calculate Jaccard similarity of signature vector of orginal doc.\n",
        "\n",
        "# default setting is .6 but as you see the result shows up to .90+\n",
        "t = 0.6\n",
        "total_over_85=[]\n",
        "\n",
        "# five_ori_signaturematrix = 5개의 시그니쳐 매트릭스중 하나\n",
        "# 5 different kind of L(50,100,200,500,1000) run 5 times\n",
        "for index,five_ori_signaturematrix in enumerate(original_signature_list):\n",
        "        # if L=50, (50,95), one of the fifth signature matrix\n",
        "        ninefive_sus_signaturematrix = signature_matrix_list[index]\n",
        "        # access by row(amount of total shingles)\n",
        "        for ori_counter in range(five_ori_signaturematrix.shape[1]):\n",
        "                #(50,) \n",
        "                # one document signaturematrix\n",
        "                one_ori_signaturematrix = five_ori_signaturematrix[:,ori_counter]\n",
        "                for sus_counter in range(ninefive_sus_signaturematrix.shape[1]):\n",
        "                          one_sus_signaturematrix = ninefive_sus_signaturematrix[:,sus_counter]\n",
        "                          # countX is increament when one original signature vector and one suspicious signature vector is match\n",
        "                          countX = 0\n",
        "                          # countY is increament when one original signature vector and one suspicious signature vector is not match\n",
        "                          countY = 0\n",
        "                          # print(one_ori_signaturematrix)\n",
        "                          # print(one_sus_signaturematrix)\n",
        "                          # break\n",
        "                          for compare_coutner, one_sus_value in enumerate(one_sus_signaturematrix):\n",
        "                            #  print(one_ori_signaturematrix.shape)\n",
        "                            #  print(one_sus_signaturematrix.shape)\n",
        "                            #  break\n",
        "                            # check by row so do samething on original \n",
        "                             one_ori_value = one_ori_signaturematrix[compare_coutner]\n",
        "                            #  print(one_sus_value)\n",
        "                            #  print(one_ori_value)\n",
        "                            #  break\n",
        "                             if one_sus_value == one_ori_value:\n",
        "                               countX = countX+1\n",
        "                             else:\n",
        "                               countY = countY + 1\n",
        "                          # similiarity equation(intersecion of two matrix/ union of two matrix)\n",
        "                          jaccard_similiarity = countX/(countX+countY)\n",
        "                          # if the similiarity is greater than t value\n",
        "                          if jaccard_similiarity > t:\n",
        "                                # save into total list\n",
        "                                temp=[filename_orilist[ori_counter],filename_suslist[sus_counter],jaccard_similiarity,five_ori_signaturematrix.shape[0]]\n",
        "                                total_over_85.append(temp)\n",
        "            \n",
        "\n",
        "# STEP-4: Facts Checks\n",
        "# For each L = {50,100,200,500,1000}, report all documents (file_names) below that have Jaccard similarity > t=0.85\n",
        "# Sort the documents in decreasing order of the Jaccard similarity\n",
        "\n",
        "\n",
        "# sort by similiarity with descending order\n",
        "sorted_over85=sorted(total_over_85,reverse=True, key = lambda a: a[2])\n",
        "for i in sorted_over85:\n",
        "    print(\"original document : \",i[0],\" suspicious document : \",i[1],\" similiarity : \",i[2],\" L = \",i[3])     \n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original document :  orig_taska.txt  suspicious document :  g0pE_taska.txt  similiarity :  0.94  L =  50\n",
            "original document :  orig_taska.txt  suspicious document :  g0pE_taska.txt  similiarity :  0.92  L =  100\n",
            "original document :  orig_taska.txt  suspicious document :  g0pE_taska.txt  similiarity :  0.902  L =  500\n",
            "original document :  orig_taska.txt  suspicious document :  g0pE_taska.txt  similiarity :  0.892  L =  1000\n",
            "original document :  orig_taska.txt  suspicious document :  g0pE_taska.txt  similiarity :  0.885  L =  200\n",
            "original document :  orig_taskd.txt  suspicious document :  g4pC_taskd.txt  similiarity :  0.84  L =  100\n",
            "original document :  orig_taskd.txt  suspicious document :  g3pA_taskd.txt  similiarity :  0.82  L =  50\n",
            "original document :  orig_taska.txt  suspicious document :  g4pC_taska.txt  similiarity :  0.82  L =  100\n",
            "original document :  orig_taska.txt  suspicious document :  g4pC_taska.txt  similiarity :  0.808  L =  500\n",
            "original document :  orig_taska.txt  suspicious document :  g4pC_taska.txt  similiarity :  0.806  L =  1000\n",
            "original document :  orig_taska.txt  suspicious document :  g4pC_taska.txt  similiarity :  0.795  L =  200\n",
            "original document :  orig_taskd.txt  suspicious document :  g3pA_taskd.txt  similiarity :  0.781  L =  1000\n",
            "original document :  orig_taskd.txt  suspicious document :  g3pA_taskd.txt  similiarity :  0.776  L =  500\n",
            "original document :  orig_taska.txt  suspicious document :  g4pC_taska.txt  similiarity :  0.76  L =  50\n",
            "original document :  orig_taskd.txt  suspicious document :  g4pC_taskd.txt  similiarity :  0.76  L =  50\n",
            "original document :  orig_taskd.txt  suspicious document :  g4pC_taskd.txt  similiarity :  0.742  L =  500\n",
            "original document :  orig_taskd.txt  suspicious document :  g3pA_taskd.txt  similiarity :  0.74  L =  100\n",
            "original document :  orig_taskd.txt  suspicious document :  g4pC_taskd.txt  similiarity :  0.724  L =  1000\n",
            "original document :  orig_taskd.txt  suspicious document :  g3pA_taskd.txt  similiarity :  0.7  L =  200\n",
            "original document :  orig_taskd.txt  suspicious document :  g4pC_taskd.txt  similiarity :  0.675  L =  200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kjCw775fpUx"
      },
      "source": [
        "### STEP - 3: LSH (30 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6q3ZaMWfpUx"
      },
      "source": [
        "# Type your code here to hash signature matrix into B buckets\n",
        "# Use the technique to split the signature matrix into b bands of r rows\n",
        "# Convert only the signature matrix generated with L=1000\n",
        "\n",
        "# base setting for LSH all explaination is given above\n",
        "b = 50\n",
        "r = 20\n",
        "B = 199\n",
        "\n",
        "N = len(shingles_total)\n",
        "permutation =get_hash_functions(N,1000)\n",
        "\n",
        "sus_shingles_total = gettotalKshingles(preprocessing_suslist,5)\n",
        "sus_shingles_per_file = getKshingles_perfile(preprocessing_suslist,5)\n",
        "sus_signature_matrix = makeSignatureMatrix(sus_shingles_per_file,sus_shingles_total,permutation)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ff-9OlU1J6h"
      },
      "source": [
        "# same preprocssing logic like previous cases but this is for one file preprocessing\n",
        "\n",
        "oridir = '/content/Original_Sample'\n",
        "def onefile_preprocessing(file,oridir):\n",
        "    with open(oridir+\"/\"+file,'r',encoding = \"ISO-8859-1\") as file1:\n",
        "      file1 = file1.read()\n",
        "      file1 = file1.replace('\\n','')\n",
        "      # Switch upper->lower\n",
        "      preprocessing = file1.lower()\n",
        "      preprocessing = re.sub('[^a-z0-9 ]+ ','',preprocessing)\n",
        "      # # remove special character\n",
        "      pattern = r'[' + string.punctuation + ']'\n",
        "      preprocessing = re.sub(pattern,'',preprocessing)\n",
        "      # # remove latin word\n",
        "      preprocessing = re.sub(r'[^\\x00-\\x7f]',r'',preprocessing)\n",
        "    return [preprocessing]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3Vu_Uf4fpUx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec54d69c-567d-484c-b190-c1004730a7f0"
      },
      "source": [
        "from random import sample\n",
        "# Type your code here to do generate candidate documents\n",
        "\n",
        "# this def make input and signature matrix\n",
        "\n",
        "\n",
        "# slice the signature matrix by given band and row\n",
        "# since L=1000 which means row is 1000. one band = 20 rows and that band will have 50 bands.\n",
        "def make_band_row_matrix(sus_signature_matrix,r):\n",
        "  row_counter=1\n",
        "  totalband = []\n",
        "  oneband=[]\n",
        "  # run sus_signature_matrix by one row\n",
        "  for one_row in sus_signature_matrix:\n",
        "    # since oneband = 20 band, make sublist(oneband) and append row 20 times\n",
        "    if row_counter < r :\n",
        "      oneband.append(one_row)\n",
        "      row_counter = row_counter + 1\n",
        "    # if one band full with 20 rows, reset the sublist(one band)  and append into totalband list.\n",
        "    else:\n",
        "      oneband.append(one_row)\n",
        "      # reset row_counter to use for next band\n",
        "      row_counter = 1\n",
        "      totalband.append(oneband)\n",
        "      # reset oneband to use for next band\n",
        "      oneband = []\n",
        "    \n",
        "  return totalband\n",
        "# logic is exactly same as below def but this is for suspcious bucket.\n",
        "def assign_sus_to_bucket(band_row_matrix,r,B,random_one_value_list):\n",
        "  sus_into_bucket_dict = dict()\n",
        "  # since band_row_matrix has 50 bands want to check by one band\n",
        "  for oneband in band_row_matrix:\n",
        "    # convert list to numpy.array\n",
        "    oneband = np.array(oneband)\n",
        "    # oneband = (20,95)\n",
        "    total_col = len(oneband[0])#95\n",
        "    # run 95 times since we have 95 suspcious document\n",
        "    for col in range(total_col):\n",
        "      # initialize rixai\n",
        "      total_rixai = 0\n",
        "      one_doc_rows=oneband[:,col]#(20,1)\n",
        "      # loop one value in the 20 rows\n",
        "      for index,one_value in enumerate(one_doc_rows):\n",
        "        # this process is pretty straight forward because this is given in the instruction. \n",
        "        ri = one_value\n",
        "        ai = random_one_value_list[index]\n",
        "        rixai = ri * ai\n",
        "        total_rixai = total_rixai + rixai\n",
        "      h = total_rixai%B# this will be address for bucket. bucket 1~199\n",
        "      # if bucket address(h) is not exist in the bucket,\n",
        "      if h not in sus_into_bucket_dict:\n",
        "        # create key = h, value = empty\n",
        "        sus_into_bucket_dict[h] = []\n",
        "      # if bucket address is existed in the ori_into_bucket_dict,\n",
        "      if col in sus_into_bucket_dict:\n",
        "      # do nothing\n",
        "        continue\n",
        "      # put filename for better visualization into key = h\n",
        "      else:\n",
        "        sus_into_bucket_dict[h].append(col)\n",
        "      # print(h)\n",
        "      # break\n",
        "    # print(sus_into_bucket_dict)\n",
        "    # break\n",
        "  return sus_into_bucket_dict\n",
        "\n",
        "# logic is exactly same as above def but this is for orignal bucket.\n",
        "def assign_ori_to_bucket(band_row_matrix,filename,r,B,random_one_value_list):\n",
        "  ori_into_bucket_dict = dict()\n",
        "  # since band_row_matrix has 50 bands want to check by one band\n",
        "  for oneband in band_row_matrix:\n",
        "    # convert list to numpy.array\n",
        "    oneband = np.array(oneband)\n",
        "    # oneband = (20,95)\n",
        "    total_col = len(oneband[0])#95\n",
        "    # run 95 times since we have 95 suspcious document\n",
        "    for col in range(total_col):\n",
        "      total_rixai = 0\n",
        "      one_doc_rows=oneband[:,col]#(20,1)\n",
        "      # loop one value in the 20 rows\n",
        "      for index,one_value in enumerate(one_doc_rows):\n",
        "        # this process is pretty straight forward because this is given in the instruction. \n",
        "        ri = one_value\n",
        "        ai = random_one_value_list[index]\n",
        "        rixai = ri * ai\n",
        "        total_rixai = total_rixai + rixai\n",
        "      h = total_rixai%B# this will be address for bucket. bucket 1~199\n",
        "      # if bucket address(h) is not exist in the bucket,\n",
        "      if h not in ori_into_bucket_dict:\n",
        "      # create key = h, value = empty\n",
        "        ori_into_bucket_dict[h] = []\n",
        "      # if bucket address is existed in the ori_into_bucket_dict,\n",
        "      if filename in ori_into_bucket_dict:\n",
        "      # do nothing\n",
        "        continue\n",
        "      # put filename for better visualization into key = h\n",
        "      else:\n",
        "        ori_into_bucket_dict[h].append(filename)\n",
        "      # print(h)\n",
        "      # break\n",
        "    # print(ori_into_bucket_dict)\n",
        "    # break\n",
        "  return ori_into_bucket_dict\n",
        "\n",
        "def jaccard_similiarity(sus_signature_matrix, ori_signature_matrix ,candidates_document_list ,t):\n",
        "  # initialize false positive and false negative count\n",
        "  count_False_Positive = 0\n",
        "  count_False_Negative = 0\n",
        "  candidates_document_list_range = len(candidates_document_list)\n",
        "  # there should be multiple candidate document in the list so read by one document\n",
        "  for one_document_counter in range(candidates_document_list_range):\n",
        "    # strucutre of logic is similiar as previous jarcard smiliarity in min hashing\n",
        "    one_document = candidates_document_list[one_document_counter]\n",
        "    # countX is increament when one original signature vector and one suspicious signature vector is match\n",
        "    countX = 0\n",
        "    # countY is increament when one original signature vector and one suspicious signature vector is not match\n",
        "    countY = 0\n",
        "    for index, one_sus_signature_vector in enumerate(sus_signature_matrix[:,one_document]): \n",
        "      # if one ori_signature_vector == one_sus_signature_vector\n",
        "      if ori_signature_matrix[:,0][index] == one_sus_signature_vector:\n",
        "        countX = countX + 1\n",
        "      else:\n",
        "        countY = countY + 1\n",
        "    # similiarity equation(intersecion of two matrix/ union of two matrix)\n",
        "    jaccard_similarity = countX/(countX+countY)\n",
        "    # if similiartity is less than equal to t value,\n",
        "    if jaccard_similarity <= t:\n",
        "      # increament count_False_Positive\n",
        "      count_False_Positive = count_False_Positive + 1\n",
        "    # if similiartity is greater than t value,\n",
        "    else:\n",
        "      # increament count_False_Negative\n",
        "      count_False_Negative = count_False_Negative + 1\n",
        "  return count_False_Positive, count_False_Negative\n",
        "\n",
        "\n",
        "# this list provide amount number of first argument without duplication\n",
        "random_one_value_list = sample(range(r),r)\n",
        "# make suspicious matrix using suspicious signature matrix\n",
        "sus_band_row_matrix = make_band_row_matrix(sus_signature_matrix,r)\n",
        "# using sus_band_row_matrix to make suspcious bucket\n",
        "sus_to_bucket = assign_sus_to_bucket(sus_band_row_matrix,r,B,random_one_value_list)\n",
        "\n",
        "#read one orignal filename in the filename_orilist\n",
        "for one_ori_filename in filename_orilist:\n",
        "  # preprocessing one orignal document\n",
        "  onefile_preprocessed = onefile_preprocessing(one_ori_filename,oridir)\n",
        "  # make shingles total for one original document\n",
        "  ori_shingles_total = gettotalKshingles(onefile_preprocessed,5)\n",
        "  # make shingles perfile(there will be only one but need for to reuse def) for one original document\n",
        "  ori_shingles_per_file = getKshingles_perfile(onefile_preprocessed,5)\n",
        "  # make one original signature matrix\n",
        "  ori_signature_matrix = makeSignatureMatrix(ori_shingles_per_file,sus_shingles_total,permutation)\n",
        "  # make original matrix using original signature matrix\n",
        "  ori_band_row_matrix = make_band_row_matrix(ori_signature_matrix,r)\n",
        "  # using ori_band_row_matrix to make original bucket\n",
        "  ori_to_bucket = assign_ori_to_bucket(ori_band_row_matrix,one_ori_filename,r,B,random_one_value_list)\n",
        "\n",
        "  #initialize candidate list\n",
        "  candidate_list = []\n",
        "\n",
        "  # compare original bucket and suspcious bucket\n",
        "  for key in ori_to_bucket.keys():\n",
        "    #if original key is in the suspcious bucket dictionary\n",
        "    if key in sus_to_bucket.keys():\n",
        "      # add into candidate list\n",
        "      candidate_list.extend(sus_to_bucket[key])\n",
        "  # convert to list to set that make sure there is no duplication\n",
        "  candidates_document = set(candidate_list)\n",
        "\n",
        "  candidates_document_list = list(candidates_document)\n",
        "  # print(ListOfcandidatesDoc)\n",
        "  t=0.6\n",
        "  print(one_ori_filename,\"\\n\")\n",
        "  false_positive, false_negative = jaccard_similiarity(sus_signature_matrix,ori_signature_matrix,candidates_document_list,t)\n",
        "  print(\"False Positives: \",false_positive)\n",
        "  print(\"False Negatives: \", false_negative)\n",
        "  print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# Follow all steps from STEP - 2 fact check (except the Jaccard similarity part)\n",
        "\n",
        "# STEP - 1: Split each original document signature vector into b bands of r rows\n",
        "\n",
        "# STEP - 2: Hash using the same hash functions created for \n",
        "    # signature matrix hashing (in the previous cell)\n",
        "    "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "orig_taska.txt \n",
            "\n",
            "False Positives:  26\n",
            "False Negatives:  1\n",
            "\n",
            "\n",
            "orig_taskb.txt \n",
            "\n",
            "False Positives:  31\n",
            "False Negatives:  0\n",
            "\n",
            "\n",
            "orig_taskc.txt \n",
            "\n",
            "False Positives:  31\n",
            "False Negatives:  0\n",
            "\n",
            "\n",
            "orig_taskd.txt \n",
            "\n",
            "False Positives:  34\n",
            "False Negatives:  1\n",
            "\n",
            "\n",
            "orig_taske.txt \n",
            "\n",
            "False Positives:  34\n",
            "False Negatives:  0\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfBzh0x_fpUx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MwRAnYHfpUy"
      },
      "source": [
        "# Type your code here to do the fact check\n",
        "# Calculate Jaccard similarity of the oiginal document with only \n",
        "    # candidate documents\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "dO8WZcU_fpUy"
      },
      "source": [
        "Report all documents (file_names) below that have Jaccard similarity > t=0.85\n",
        "Sort the documents in decreasing order of the Jaccard similarity\n",
        "\n",
        "\n",
        "original document :  orig_taska.txt  suspicious document :  g0pE_taska.txt  similiarity :  0.91  L =  200\n",
        "original document :  orig_taska.txt  suspicious document :  g0pE_taska.txt  similiarity :  0.88  L =  100\n",
        "original document :  orig_taskd.txt  suspicious document :  g3pA_taskd.txt  similiarity :  0.88  L =  100\n",
        "original document :  orig_taska.txt  suspicious document :  g0pE_taska.txt  similiarity :  0.868  L =  1000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcgpTZQHfpUy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "O6UtnmCXfpUy"
      },
      "source": [
        "Report the list of false positives and false negatives below\n",
        "\n",
        "orig_taska.txt\n",
        "False Positives:  24\n",
        "False Negatives:  2\n",
        "\n",
        "\n",
        "orig_taskb.txt\n",
        "False Positives:  28\n",
        "False Negatives:  0\n",
        "\n",
        "\n",
        "orig_taskc.txt\n",
        "False Positives:  28\n",
        "False Negatives:  0\n",
        "\n",
        "\n",
        "orig_taskd.txt\n",
        "False Positives:  27\n",
        "False Negatives:  1\n",
        "\n",
        "\n",
        "orig_taske.txt\n",
        "False Positives:  29\n",
        "False Negatives:  0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fKuJGPbfpUy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}